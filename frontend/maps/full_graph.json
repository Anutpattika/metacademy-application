{"distributed_representations":{"title":"Distributed representations","dependencies":[{"from_tag":"feed-forward-neural-nets","to_tag":"distributed-representations","reason":"Feed-forward neural nets are an instructive example of a distributed representation."}],"pointers":[]},"factor_analysis":{"title":"Factor analysis","dependencies":[{"from_tag":"expectation-maximization","to_tag":"factor-analysis","reason":"We can use EM to learn the parameters."}],"pointers":[{"from_tag":"factor-analysis","to_tag":"principal-component-analysis","blurb":"** Principal component analysis (PCA), which finds the maximum variance directions by solving an eigenvalue problem"},{"from_tag":"factor-analysis","to_tag":"probabilistic-pca","blurb":"** probabilistic PCA, a similar generative model, but where the noise covariance is spherical rather than diagonal"}]},"pac_learning_proofs":{"title":"PAC learning: proofs","dependencies":[{"from_tag":"pac-learning","to_tag":"pac-learning-proofs","reason":"None"}],"pointers":[]},"induced_factorizations":{"title":"Induced factorizations","dependencies":[{"from_tag":"variational-inference","to_tag":"induced-factorizations","reason":"None"}],"pointers":[]},"fisher_kernel":{"title":"Fisher kernel","dependencies":[{"from_tag":"constructing-kernels","to_tag":"fisher-kernel","reason":"None"}],"pointers":[]},"adaptive_rejection_sampling":{"title":"Adaptive rejection sampling","dependencies":[{"from_tag":"rejection-sampling","to_tag":"adaptive-rejection-sampling","reason":"None"}],"pointers":[]},"lasso":{"title":"LASSO","dependencies":[{"from_tag":"linear-regression","to_tag":"lasso","reason":"LASSO is a regularized form of linear regression."}],"pointers":[{"from_tag":"lasso","to_tag":"stochastic-gradient-descent","blurb":"** stochastic gradient descent"}]},"generative_vs_discriminative":{"title":"Generative vs. discriminative models","dependencies":[{"from_tag":"gaussian-discriminant-analysis","to_tag":"generative-vs-discriminative","reason":"GDA is an instructive example of a generative model."},{"from_tag":"logistic-regression","to_tag":"generative-vs-discriminative","reason":"Logistic regression is an instructive example of a discriminative model."},{"from_tag":"naive-bayes","to_tag":"generative-vs-discriminative","reason":"Naive Bayes is an instructive example of a generative model."},{"from_tag":"generalization","to_tag":"generative-vs-discriminative","reason":"Generalization and the bias/variance tradoff are important issues in comparing generative and discriminative models."}],"pointers":[{"from_tag":"generative-vs-discriminative","to_tag":"naive-bayes","blurb":"** naive Bayes"},{"from_tag":"generative-vs-discriminative","to_tag":"gaussian-discriminant-analysis","blurb":"** Gaussian discriminant analysis"},{"from_tag":"generative-vs-discriminative","to_tag":"linear-regression-as-maximum-likelihood","blurb":"** linear regression"},{"from_tag":"generative-vs-discriminative","to_tag":"logistic-regression","blurb":"** logistic regression"},{"from_tag":"generative-vs-discriminative","to_tag":"feed-forward-neural-nets","blurb":"** feed-forward neural networks"},{"from_tag":"generative-vs-discriminative","to_tag":"gaussian-process-regression","blurb":"** Gaussian process regression"}]},"structural_risk_minimization":{"title":"Structural risk minimization","dependencies":[{"from_tag":"vc-dimension","to_tag":"structural-risk-minimization","reason":"Structural risk minimization is based on a nested sequence of models of increasing VC dimension."}],"pointers":[{"from_tag":"structural-risk-minimization","to_tag":"model-selection","blurb":"** model selection"}]},"svm_optimality_conditions":{"title":"SVM optimality conditions","dependencies":[{"from_tag":"support-vector-machine","to_tag":"svm-optimality-conditions","reason":"None"}],"pointers":[]},"gibbs_sampling":{"title":"Gibbs sampling","dependencies":[{"from_tag":"markov-chain-monte-carlo","to_tag":"gibbs-sampling","reason":"Gibbs sampling is an MCMC algorithm."}],"pointers":[{"from_tag":"gibbs-sampling","to_tag":"gibbs-as-mh","blurb":"* Gibbs sampling can be viewed as a special case of Metropolis-Hastings."},{"from_tag":"gibbs-sampling","to_tag":"slice-sampling","blurb":"* Slice sampling is a special case of Gibbs sampling, good for sampling from univariate distributions with no closed-form sampler"}]},"least_squares_for_classification":{"title":"Least squares for classification","dependencies":[{"from_tag":"binary-linear-classifiers","to_tag":"least-squares-for-classification","reason":"This is an example of a binary linear classifier."},{"from_tag":"linear-regression","to_tag":"least-squares-for-classification","reason":"None"}],"pointers":[{"from_tag":"least-squares-for-classification","to_tag":"logistic-regression","blurb":"** logistic regression"},{"from_tag":"least-squares-for-classification","to_tag":"gda-is-least-squares","blurb":"* The least squares solution, surprisingly, turns out to be equivalent to Gaussian discriminant analysis."}]},"regularization_as_map_estimation":{"title":"Interpreting regularization as MAP estimation","dependencies":[],"pointers":[{"from_tag":"regularization-as-map-estimation","to_tag":"bayesian-parameter-estimation","blurb":"* MAP estimation is not a fully Bayesian method. For the Bayesian treatment, see Bayesian parameter estimation."}]},"vc_dimension":{"title":"VC dimension","dependencies":[{"from_tag":"pac-learning","to_tag":"vc-dimension","reason":"VC dimension is most often used in the context of PAC learning."}],"pointers":[{"from_tag":"vc-dimension","to_tag":"structural-risk-minimization","blurb":"* Structural risk minimization is an idea based on VC dimension which justifies algorithms such as SVMs."}]},"probit_regression":{"title":"Probit regression","dependencies":[{"from_tag":"logistic-regression","to_tag":"probit-regression","reason":"Probit regression is a variant on logistic regression."},{"from_tag":"probit-function","to_tag":"probit-regression","reason":"Probit regression is defined in terms of the probit function."}],"pointers":[]},"bayes_param_exp_fam":{"title":"Bayesian parameter estimation in exponential families","dependencies":[{"from_tag":"bayesian-parameter-estimation","to_tag":"bayes-param-exp-fam","reason":"None"},{"from_tag":"bayesian-parameter-estimation-multinomial","to_tag":"bayes-param-exp-fam","reason":"The multinomial distribution is an instructive example."},{"from_tag":"maximum-likelihood-in-exponential-families","to_tag":"bayes-param-exp-fam","reason":"Bayesian parameter estimation has a similar form to maximum likelihood estimation."}],"pointers":[]},"bayesian_neural_nets":{"title":"Bayesian neural nets","dependencies":[{"from_tag":"bayesian-linear-regression","to_tag":"bayesian-neural-nets","reason":"Bayesian neural nets are based on the same ideas as Bayesian linear regression."},{"from_tag":"laplace-approximation","to_tag":"bayesian-neural-nets","reason":"The Laplace approximation is a way of approximating Bayesian neural nets."},{"from_tag":"evidence-approximation","to_tag":"bayesian-neural-nets","reason":"The evidence approximation is a way of fitting the hyperparameters of a Bayesian neural net."}],"pointers":[{"from_tag":"bayesian-neural-nets","to_tag":"bayesian-neural-nets-as-gaussian-processes","blurb":"* Bayesian neural nets converge to Gaussian processes in the limit of infinitely many units."}]},"generalized_linear_models":{"title":"Generalized linear models","dependencies":[{"from_tag":"linear-regression","to_tag":"generalized-linear-models","reason":"Linear regression is an instructive example of generalized linear models."},{"from_tag":"logistic-regression","to_tag":"generalized-linear-models","reason":"Logistic regression is an instructive example of generalized linear models."}],"pointers":[{"from_tag":"generalized-linear-models","to_tag":"linear-regression","blurb":"** linear regression"},{"from_tag":"generalized-linear-models","to_tag":"logistic-regression","blurb":"** logistic regression"},{"from_tag":"generalized-linear-models","to_tag":"multinomial-logistic-regression","blurb":"** multinomial logistic regression"}]},"ais_partition_function":{"title":"Estimating the partition function using AIS","dependencies":[{"from_tag":"sampling-partition-function","to_tag":"ais-partition-function","reason":"None"},{"from_tag":"annealed-importance-sampling","to_tag":"ais-partition-function","reason":"None"}],"pointers":[{"from_tag":"ais-partition-function","to_tag":"variational-bayes","blurb":"** variational Bayes"},{"from_tag":"ais-partition-function","to_tag":"expectation-propagation","blurb":"** expectation propagation"}]},"naive_bayes":{"title":"Naive Bayes","dependencies":[{"from_tag":"maximum-likelihood","to_tag":"naive-bayes","reason":"Naive Bayes is fit using maximum likelihood."},{"from_tag":"multiway-classification","to_tag":"naive-bayes","reason":"Naive Bayes is often used for multiway classification."},{"from_tag":"binary-linear-classifiers","to_tag":"naive-bayes","reason":"Naive Bayes is a linear classifier."}],"pointers":[{"from_tag":"naive-bayes","to_tag":"generative-vs-discriminative","blurb":"* Naive Bayes is a generative model."},{"from_tag":"naive-bayes","to_tag":"perceptron","blurb":"** perceptron"},{"from_tag":"naive-bayes","to_tag":"logistic-regression","blurb":"** logistic regression"},{"from_tag":"naive-bayes","to_tag":"gaussian-discriminant-analysis","blurb":"** Gaussian discriminant analysis"}]},"early_stopping":{"title":"Early stopping","dependencies":[{"from_tag":"backpropagation","to_tag":"early-stopping","reason":"Early stopping is commonly applied to the backpropagation algorithm."},{"from_tag":"generalization","to_tag":"early-stopping","reason":"Early stopping is meant to improve generalization performance."}],"pointers":[{"from_tag":"early-stopping","to_tag":"weight-decay","blurb":"** Weight decay, a form of $L_2$ regularization"},{"from_tag":"early-stopping","to_tag":"tikhonov-regularization","blurb":"** Tikhonov regularization, which rewards invariance to noise in the inputs"},{"from_tag":"early-stopping","to_tag":"tangent-propagation","blurb":"** Tangent propagation, which rewards invariance to irrelevant transformations of the inputs such as translation and scalling"}]},"bayesian_model_comparison":{"title":"Bayesian model comparison","dependencies":[{"from_tag":"bayesian-parameter-estimation","to_tag":"bayesian-model-comparison","reason":"Most techniques for Bayesian model comparison involve estimating the parameters as well."},{"from_tag":"model-selection","to_tag":"bayesian-model-comparison","reason":"Bayesian model comparison is one method for model selection."}],"pointers":[{"from_tag":"bayesian-model-comparison","to_tag":"variational-bayes","blurb":"** variational Bayes"}]},"maximum_likelihood":{"title":"Maximum likelihood","dependencies":[],"pointers":[{"from_tag":"maximum-likelihood","to_tag":"linear-regression-as-maximum-likelihood","blurb":"** Linear regression"},{"from_tag":"maximum-likelihood","to_tag":"logistic-regression","blurb":"** Logistic regression"},{"from_tag":"maximum-likelihood","to_tag":"mixture-of-gaussians","blurb":"** Mixture of Gaussians modeling"},{"from_tag":"maximum-likelihood","to_tag":"generalization","blurb":"* Maximum likelihood can be prone to overfitting when there is not enough data to estimate the parameters."},{"from_tag":"maximum-likelihood","to_tag":"bayesian-parameter-estimation","blurb":"** Bayesian parameter estimation"},{"from_tag":"maximum-likelihood","to_tag":"model-selection","blurb":"** Model selection"},{"from_tag":"maximum-likelihood","to_tag":"early-stopping","blurb":"** Early stopping"},{"from_tag":"maximum-likelihood","to_tag":"expectation-maximization","blurb":"** expectation-maximization (EM)"}]},"feed_forward_neural_nets":{"title":"Feed-forward neural nets","dependencies":[{"from_tag":"basis-function-expansions","to_tag":"feed-forward-neural-nets","reason":"Feed-forward neural nets can be seen as an adaptive basis function expansion."}],"pointers":[{"from_tag":"feed-forward-neural-nets","to_tag":"distributed-representations","blurb":"* Neural nets are a form of distributed representation."},{"from_tag":"feed-forward-neural-nets","to_tag":"backpropagation","blurb":"* Neural nets can be trained using an algorithm called backpropagation."},{"from_tag":"feed-forward-neural-nets","to_tag":"convolutional-nets","blurb":"** convolutional nets, an architecture for vision problems where the weights are replicated across an image"}]},"logistic_regression":{"title":"Logistic regression","dependencies":[{"from_tag":"binary-linear-classifiers","to_tag":"logistic-regression","reason":"Logistic regression is a binary linear classifier"},{"from_tag":"linear-regression-as-maximum-likelihood","to_tag":"logistic-regression","reason":"Logistic regression is like linear regression, but with a different observation model."}],"pointers":[{"from_tag":"logistic-regression","to_tag":"generative-vs-discriminative","blurb":"* Logistic regression is a discriminative model."},{"from_tag":"logistic-regression","to_tag":"generalized-linear-models","blurb":"* Logistic regression is a kind of generalized linear model."},{"from_tag":"logistic-regression","to_tag":"logistic-regression-irls","blurb":"* Iterative reweighted least squares (IRLS) is a faster method for fitting logistic regression in low-dimensional settings."},{"from_tag":"logistic-regression","to_tag":"probit-regression","blurb":"* Probit regression is a related model which uses thresholded Gaussians for the observation model instead the logistic function."},{"from_tag":"logistic-regression","to_tag":"bayesian-logistic-regression","blurb":"* We can formulate logistic regression as a Bayesian model to prevent overfitting and measure our confidence in the answer."},{"from_tag":"logistic-regression","to_tag":"perceptron","blurb":"** perceptron"},{"from_tag":"logistic-regression","to_tag":"gaussian-discriminant-analysis","blurb":"** Gaussian discriminant analysis"},{"from_tag":"logistic-regression","to_tag":"naive-bayes","blurb":"** naive Bayes"},{"from_tag":"logistic-regression","to_tag":"generalization","blurb":"* We would like our classifier to generalize well to new data, not just the data it's already seen."},{"from_tag":"logistic-regression","to_tag":"regularization","blurb":"** regularization, where overly complex solutions are penalized"},{"from_tag":"logistic-regression","to_tag":"model-selection","blurb":"** model selection"},{"from_tag":"logistic-regression","to_tag":"basis-function-expansions","blurb":"** basis function expansions"},{"from_tag":"logistic-regression","to_tag":"feed-forward-neural-nets","blurb":"** neural networks"},{"from_tag":"logistic-regression","to_tag":"kernel-svm","blurb":"** kernel methods"}]},"backpropagation_second_order":{"title":"Backpropagation for second-order methods","dependencies":[{"from_tag":"backpropagation","to_tag":"backpropagation-second-order","reason":"None"}],"pointers":[]},"metropolis_hastings":{"title":"Metropolis-Hastings algorithm","dependencies":[{"from_tag":"markov-chain-monte-carlo","to_tag":"metropolis-hastings","reason":"M-H is an example of an MCMC algorithm."}],"pointers":[{"from_tag":"metropolis-hastings","to_tag":"gibbs-as-mh","blurb":"* Gibbs sampling is a commonly used special case of M-H."},{"from_tag":"metropolis-hastings","to_tag":"hamiltonian-monte-carlo","blurb":"** Hamiltonian Monte Carlo (HMC), which uses gradient information to sample from continuous models"}]},"probit_function":{"title":"Probit function","dependencies":[],"pointers":[{"from_tag":"probit-function","to_tag":"probit-regression","blurb":"** probit regression"}]},"gaussian_mixtures_vs_k_means":{"title":"Comparing Gaussian mixtures and k-means","dependencies":[{"from_tag":"mixture-of-gaussians","to_tag":"gaussian-mixtures-vs-k-means","reason":"None"},{"from_tag":"k-means","to_tag":"gaussian-mixtures-vs-k-means","reason":"None"}],"pointers":[{"from_tag":"gaussian-mixtures-vs-k-means","to_tag":"naive-bayes","blurb":"** naive Bayes, a probabilistic model for discrete data"}]},"sampling_partition_function":{"title":"Estimating the partition function using sampling","dependencies":[{"from_tag":"importance-sampling","to_tag":"sampling-partition-function","reason":"The most basic sampling-based estimator of the partition function uses importance sampling."}],"pointers":[{"from_tag":"sampling-partition-function","to_tag":"ais-partition-function","blurb":"** annealed importance sampling (AIS)"}]},"basis_function_expansions":{"title":"Basis function expansions","dependencies":[{"from_tag":"linear-regression","to_tag":"basis-function-expansions","reason":"Linear regression is a simple model which motivates basis function expansions, which are applicable to linear models more generally."}],"pointers":[{"from_tag":"basis-function-expansions","to_tag":"feed-forward-neural-nets","blurb":"** Neural networks, which allow the basis functions to be adapted to the data."},{"from_tag":"basis-function-expansions","to_tag":"kernel-ridge-regression","blurb":"** Kernels, which a way of implicitly representing a very high-dimensional (possibly infinite dimensional) feature expansion in terms of a kernel function between data points"}]},"wishart_distribution":{"title":"Wishart distribution","dependencies":[],"pointers":[]},"slice_sampling":{"title":"Slice sampling","dependencies":[{"from_tag":"gibbs-sampling","to_tag":"slice-sampling","reason":"Slice sampling is a special case of Gibbs sampling"}],"pointers":[{"from_tag":"slice-sampling","to_tag":"hamiltonian-monte-carlo","blurb":"* Hamiltonian Monte Carlo (HMC) is a special case of slice sampling which uses gradient information to sample from a continuous model"}]},"nonparametric_density_estimation":{"title":"Nonparametric density estimation","dependencies":[],"pointers":[{"from_tag":"nonparametric-density-estimation","to_tag":"mixture-of-gaussians","blurb":"* Mixture of Gaussians models are a more compact parametric representation which has the same form."}]},"mixture_of_gaussians":{"title":"Mixture of Gaussians models","dependencies":[],"pointers":[{"from_tag":"mixture-of-gaussians","to_tag":"k-means","blurb":"* K-means is a simpler clustering model which is faster to fit and often used as an initialization."}]},"ml_multivariate_gaussians":{"title":"Maximum likelihood: multivariate Gaussians","dependencies":[{"from_tag":"maximum-likelihood","to_tag":"ml-multivariate-gaussians","reason":"None"}],"pointers":[{"from_tag":"ml-multivariate-gaussians","to_tag":"principal-component-analysis","blurb":"** principal component analysis (PCA)"},{"from_tag":"ml-multivariate-gaussians","to_tag":"factor-analysis","blurb":"** factor analysis"}]},"gaussian_logistic_bound":{"title":"Gaussian lower bound on logistic function","dependencies":[{"from_tag":"variational-inference-convex-duality","to_tag":"gaussian-logistic-bound","reason":"The Gaussian lower bound is inspired by convex duality."}],"pointers":[{"from_tag":"gaussian-logistic-bound","to_tag":"variational-logistic-regression","blurb":"** logistic regression"}]},"independent_component_analysis":{"title":"Independent component analysis","dependencies":[],"pointers":[]},"linear_regression":{"title":"Linear regression","dependencies":[],"pointers":[{"from_tag":"linear-regression","to_tag":"multiway-classification","blurb":"** categorical"},{"from_tag":"linear-regression","to_tag":"generalization","blurb":"* Vanilla linear regression is prone to over-fitting."},{"from_tag":"linear-regression","to_tag":"lasso","blurb":"** L1-regularized linear regression (Lasso)"},{"from_tag":"linear-regression","to_tag":"pca-preprocessing","blurb":"** PCA preprocessing"},{"from_tag":"linear-regression","to_tag":"model-selection","blurb":"** Model selection"},{"from_tag":"linear-regression","to_tag":"support-vector-regression","blurb":"** support vector regression"},{"from_tag":"linear-regression","to_tag":"basis-function-expansions","blurb":"** basis function expansions"},{"from_tag":"linear-regression","to_tag":"feed-forward-neural-nets","blurb":"** neural networks"},{"from_tag":"linear-regression","to_tag":"kernel-ridge-regression","blurb":"** kernel methods"},{"from_tag":"linear-regression","to_tag":"linear-regression-as-maximum-likelihood","blurb":"* Linear regression can be interpreted as maximum likelihood estimation under a Gaussian noise model."}]},"logistic_regression_irls":{"title":"Fitting logistic regression with iterative reweighted least squares","dependencies":[{"from_tag":"logistic-regression","to_tag":"logistic-regression-irls","reason":"None"}],"pointers":[{"from_tag":"logistic-regression-irls","to_tag":"stochastic-gradient-descent","blurb":"** For large numbers of data points, consider stochastic gradient descent."}]},"robbins_monro":{"title":"Robbins-Monro procedure","dependencies":[],"pointers":[{"from_tag":"robbins-monro","to_tag":"stochastic-gradient-descent","blurb":"* Stochastic gradient descent is an optimization algorithm where the Robbins-Monro procedure is often applied."},{"from_tag":"robbins-monro","to_tag":"sequential-maximum-likelihood-estimation","blurb":"* Robbins-Monro can be used for sequential maximum likelihood estimation."}]},"bayesian_neural_nets_as_gaussian_processes":{"title":"Bayesian neural nets converge to Gaussian processes","dependencies":[{"from_tag":"bayesian-neural-nets","to_tag":"bayesian-neural-nets-as-gaussian-processes","reason":"None"},{"from_tag":"gaussian-processes","to_tag":"bayesian-neural-nets-as-gaussian-processes","reason":"None"}],"pointers":[]},"variational_inference":{"title":"Variational inference","dependencies":[],"pointers":[{"from_tag":"variational-inference","to_tag":"expectation-propagation","blurb":"** Expectation propagation, which is slower, but often considerably more accurate, than mean field"},{"from_tag":"variational-inference","to_tag":"variational-exponential-family","blurb":"* Variational inference works out nicely when the model is built out of exponential family distributions."},{"from_tag":"variational-inference","to_tag":"variational-bayes","blurb":"* Variational Bayes is the application of variational inference to fitting Bayesian models."},{"from_tag":"variational-inference","to_tag":"markov-chain-monte-carlo","blurb":"* Markov chain Monte Carlo (MCMC) is another versatile set of techniques for performing inference in probabilistic models."}]},"support_vector_machine":{"title":"The support vector machine","dependencies":[],"pointers":[{"from_tag":"support-vector-machine","to_tag":"sequential-minimal-optimization","blurb":"* The SVM can be optimized with the sequential minimal optimization (SMO) algorithm."},{"from_tag":"support-vector-machine","to_tag":"kernel-svm","blurb":"* The main advantage of SVMs is that they can be kernelized in order to capture nonlinear dependencies."}]},"weight_decay":{"title":"Weight decay","dependencies":[{"from_tag":"backpropagation","to_tag":"weight-decay","reason":"Weight decay is used as part of the backpropagation algorithm."}],"pointers":[{"from_tag":"weight-decay","to_tag":"regularization","blurb":"* Weight decay is an example of a regularization method."},{"from_tag":"weight-decay","to_tag":"tikhonov-regularization","blurb":"** Tikhonov regularization, which rewards invariance to noise in the inputs"},{"from_tag":"weight-decay","to_tag":"tangent-propagation","blurb":"** Tangent propagation, which rewards invariance to irrelevant transformations of the inputs such as translation and scalling"},{"from_tag":"weight-decay","to_tag":"early-stopping","blurb":"* Early stopping is another strategy to prevent overfitting in neural nets."}]},"sampling_gaussian":{"title":"Sampling from a Gaussian","dependencies":[{"from_tag":"transformation-method","to_tag":"sampling-gaussian","reason":"This trick uses the transformation method."}],"pointers":[]},"gaussian_process_regression":{"title":"Gaussian process regression","dependencies":[{"from_tag":"gaussian-processes","to_tag":"gaussian-process-regression","reason":"None"},{"from_tag":"bayesian-linear-regression","to_tag":"gaussian-process-regression","reason":"Gaussian process regression is a kernelized version of Bayesian linear regression."},{"from_tag":"kernel-ridge-regression","to_tag":"gaussian-process-regression","reason":"Gaussian process regression is a Bayesian version of kernel ridge regression."}],"pointers":[{"from_tag":"gaussian-process-regression","to_tag":"gaussian-process-classification","blurb":"** classification"},{"from_tag":"gaussian-process-regression","to_tag":"constructing-kernels","blurb":"* Techniques for constructing kernel functions"}]},"bayesian_parameter_estimation_multinomial":{"title":"Bayesian parameter estimation: multinomial distribution","dependencies":[{"from_tag":"bayesian-parameter-estimation","to_tag":"bayesian-parameter-estimation-multinomial","reason":"This is an example of Bayesian parameter estimation."},{"from_tag":"dirichlet-distribution","to_tag":"bayesian-parameter-estimation-multinomial","reason":"The Dirichlet distribution is the conjugate prior for the multinomial distribution."}],"pointers":[]},"gda_is_least_squares":{"title":"GDA is equivalent to least squares","dependencies":[{"from_tag":"least-squares-for-classification","to_tag":"gda-is-least-squares","reason":"None"},{"from_tag":"gaussian-discriminant-analysis","to_tag":"gda-is-least-squares","reason":"None"}],"pointers":[]},"expectation_propagation":{"title":"Expectation propagation","dependencies":[{"from_tag":"variational-inference","to_tag":"expectation-propagation","reason":"EP is a variational inference algorithm."}],"pointers":[{"from_tag":"expectation-propagation","to_tag":"variational-bayes","blurb":"* Variational Bayes is an alternative variational inference method which is often simpler and faster but less accurate."}]},"em_variational_interpretation":{"title":"Variational interpretation of EM","dependencies":[{"from_tag":"expectation-maximization","to_tag":"em-variational-interpretation","reason":"None"}],"pointers":[]},"variational_bayes":{"title":"Variational Bayes","dependencies":[{"from_tag":"bayesian-model-comparison","to_tag":"variational-bayes","reason":"Variational Bayes is a way of approximating Bayesian model comparison."},{"from_tag":"variational-inference","to_tag":"variational-bayes","reason":"Variational Bayes is an application of variational inference techniques."}],"pointers":[{"from_tag":"variational-bayes","to_tag":"variational-mixture-of-gaussians","blurb":"** Mixture of Gaussians"},{"from_tag":"variational-bayes","to_tag":"variational-linear-regression","blurb":"** linear regression"},{"from_tag":"variational-bayes","to_tag":"variational-logistic-regression","blurb":"** logistic regression"},{"from_tag":"variational-bayes","to_tag":"laplace-approximation","blurb":"** Laplace approximation"}]},"rejection_sampling":{"title":"Rejection sampling","dependencies":[{"from_tag":"sampling-methods","to_tag":"rejection-sampling","reason":"None"}],"pointers":[{"from_tag":"rejection-sampling","to_tag":"importance-sampling","blurb":"* Importance sampling is a way of getting weighted samples from a distribution and is useful in many of the same situations."},{"from_tag":"rejection-sampling","to_tag":"adaptive-rejection-sampling","blurb":"* Adaptive rejection sampling is a way to improve the sampler based on the rejected samples"},{"from_tag":"rejection-sampling","to_tag":"gibbs-sampling","blurb":"** Gibbs sampling, a generic and widely applicable sampling algorithm"},{"from_tag":"rejection-sampling","to_tag":"metropolis-hastings","blurb":"** Metropolis-Hastings algorithm, which is very general"}]},"von_mises_distribution":{"title":"Von Mises distribution","dependencies":[{"from_tag":"maximum-likelihood","to_tag":"von-mises-distribution","reason":"One of the main advantages of the von Mises distribution is that maximum likelihood estimation is easy."}],"pointers":[]},"bayes_param_multivariate_gaussian":{"title":"Bayesian parameter estimation: multivariate Gaussians","dependencies":[{"from_tag":"bayesian-parameter-estimation","to_tag":"bayes-param-multivariate-gaussian","reason":"None"},{"from_tag":"wishart-distribution","to_tag":"bayes-param-multivariate-gaussian","reason":"The Wishart distribution is the conjugate prior for the precision matrix of a multivariate Gaussian."},{"from_tag":"student-t-distribution","to_tag":"bayes-param-multivariate-gaussian","reason":"The predictive distribution is a student-t distribution."}],"pointers":[{"from_tag":"bayes-param-multivariate-gaussian","to_tag":"bayesian-linear-regression","blurb":"** Bayesian linear regression"},{"from_tag":"bayes-param-multivariate-gaussian","to_tag":"principal-component-analysis","blurb":"** principal component analysis (PCA)"},{"from_tag":"bayes-param-multivariate-gaussian","to_tag":"factor-analysis","blurb":"** factor analysis"}]},"multiclass_svm":{"title":"Multiclass SVM","dependencies":[{"from_tag":"support-vector-machine","to_tag":"multiclass-svm","reason":"None"},{"from_tag":"multiway-classification","to_tag":"multiclass-svm","reason":"None"}],"pointers":[{"from_tag":"multiclass-svm","to_tag":"multinomial-logistic-regression","blurb":"* Multinomial logistic regression is a multiway classification algorithm with a probabilistic interpretation."}]},"perceptron":{"title":"Perceptron algorithm","dependencies":[{"from_tag":"binary-linear-classifiers","to_tag":"perceptron","reason":"The perceptron is a binary linear classifier."}],"pointers":[{"from_tag":"perceptron","to_tag":"logistic-regression","blurb":"** logistic regression, which is formulated as a probabilistic model"}]},"convolutional_nets":{"title":"Convolutional neural nets","dependencies":[{"from_tag":"learning-invariances-in-neural-nets","to_tag":"convolutional-nets","reason":"Convolutional nets are a neural net architecture that enforces invariance to translation."}],"pointers":[{"from_tag":"convolutional-nets","to_tag":"tangent-propagation","blurb":"** tangent propagation, which penalizes instability with respect to transformations"},{"from_tag":"convolutional-nets","to_tag":"tikhonov-regularization","blurb":"** Tikhonov regularization, which penalizes instability with respect to noise"}]},"bayesian_parameter_estimation":{"title":"Bayesian parameter estimation","dependencies":[{"from_tag":"maximum-likelihood","to_tag":"bayesian-parameter-estimation","reason":"We motivate Bayesian parameter estimation by contrasting it with maximum likelihood estimation."}],"pointers":[{"from_tag":"bayesian-parameter-estimation","to_tag":"bayesian-decision-theory","blurb":"* Ultimately, we don't just want to learn parameters, we want to use them for something. Bayesian decision theory concerns how to act based on our inferences from the data."},{"from_tag":"bayesian-parameter-estimation","to_tag":"variational-bayes","blurb":"** variational Bayes, a framework for approximating intractable posterior distributions with tractable ones"},{"from_tag":"bayesian-parameter-estimation","to_tag":"markov-chain-monte-carlo","blurb":"** markov chain Monte Carlo (MCMC), a set of techniques for approximately sampling from the posterior"},{"from_tag":"bayesian-parameter-estimation","to_tag":"bayesian-model-comparison","blurb":"** Bayesian model comparison"},{"from_tag":"bayesian-parameter-estimation","to_tag":"uninformative-priors","blurb":"** uninformative priors, which try to say as little as possible about the parameter"}]},"gaussian_process_classification":{"title":"Gaussian process classification","dependencies":[{"from_tag":"gaussian-process-regression","to_tag":"gaussian-process-classification","reason":"Gaussian process classification builds on Gaussian process regression."},{"from_tag":"bayesian-logistic-regression","to_tag":"gaussian-process-classification","reason":"Gaussian process classification is a kernelized version of Bayesian logistic regression."}],"pointers":[{"from_tag":"gaussian-process-classification","to_tag":"approximating-gp-classification","blurb":"* Techniques for approximating Gaussian process classification"},{"from_tag":"gaussian-process-classification","to_tag":"gaussian-process-regression","blurb":"** regression"},{"from_tag":"gaussian-process-classification","to_tag":"constructing-kernels","blurb":"* Techniques for constructing kernel functions"}]},"gaussian_discriminant_analysis":{"title":"Gaussian discriminant analysis","dependencies":[{"from_tag":"binary-linear-classifiers","to_tag":"gaussian-discriminant-analysis","reason":"GDA is a binary linear classifier."},{"from_tag":"mixture-of-gaussians","to_tag":"gaussian-discriminant-analysis","reason":"GDA uses a mixture of Gaussians model."},{"from_tag":"maximum-likelihood","to_tag":"gaussian-discriminant-analysis","reason":"GDA is fit using maximum likelihood."}],"pointers":[{"from_tag":"gaussian-discriminant-analysis","to_tag":"generative-vs-discriminative","blurb":"* GDA is an example of a generative model."},{"from_tag":"gaussian-discriminant-analysis","to_tag":"fisher-discriminant-analysis","blurb":"* The generalization to more than two classes is known as Fisher discriminant analysis."}]},"kernel_pca":{"title":"Kernel PCA","dependencies":[{"from_tag":"principal-component-analysis","to_tag":"kernel-pca","reason":"None"},{"from_tag":"kernel-trick","to_tag":"kernel-pca","reason":"None"}],"pointers":[{"from_tag":"kernel-pca","to_tag":"feed-forward-neural-nets","blurb":"** feed-forward neural nets, which adapt the representations in the context of some other learning objective"}]},"kernel_trick":{"title":"The kernel trick","dependencies":[{"from_tag":"basis-function-expansions","to_tag":"kernel-trick","reason":"The kernel trick is a way of compactly representing very high-dimensional basis function expansions."}],"pointers":[{"from_tag":"kernel-trick","to_tag":"constructing-kernels","blurb":"* Techniques for constructing kernels"},{"from_tag":"kernel-trick","to_tag":"kernel-ridge-regression","blurb":"** kernel ridge regression"},{"from_tag":"kernel-trick","to_tag":"kernel-svm","blurb":"** kernel support vector machines"},{"from_tag":"kernel-trick","to_tag":"gaussian-process-regression","blurb":"** Gaussian process regression"}]},"markov_models":{"title":"Markov models","dependencies":[],"pointers":[{"from_tag":"markov-models","to_tag":"hidden-markov-models","blurb":"* Hidden Markov models are a widely used class of probabilistic models where the data are explained in terms of a latent Markov chain."}]},"gaussian_process_latent_variable_model":{"title":"Gaussian process latent variable model","dependencies":[{"from_tag":"gaussian-process-regression","to_tag":"gaussian-process-latent-variable-model","reason":"The GP-LVM model is closely related to Gaussian process regression."}],"pointers":[{"from_tag":"gaussian-process-latent-variable-model","to_tag":"gaussian-process-latent-variable-model","blurb":"** Gaussian process latent variable models, a Bayesian model similar in spirit to MDS"}]},"smoother_matrix":{"title":"The smoother matrix","dependencies":[{"from_tag":"generalization","to_tag":"smoother-matrix","reason":"The smoother matrix is a way of analyzing the generalization performance of linear regression."}],"pointers":[{"from_tag":"smoother-matrix","to_tag":"effective-num-parameters","blurb":"* The smoother matrix lets us define the effective number of parameters of a regression model."},{"from_tag":"smoother-matrix","to_tag":"kernel-ridge-regression","blurb":"** kernel ridge regression"},{"from_tag":"smoother-matrix","to_tag":"gaussian-process-regression","blurb":"** Gaussian process regression"}]},"approximating_gp_classification":{"title":"Approximating GP classification","dependencies":[{"from_tag":"gaussian-process-classification","to_tag":"approximating-gp-classification","reason":"None"},{"from_tag":"laplace-approximation","to_tag":"approximating-gp-classification","reason":"The Laplace approximation is one way to approximate GP classification."},{"from_tag":"logistic-regression-irls","to_tag":"approximating-gp-classification","reason":"The update rule is similar to IRLS for logistic regression."}],"pointers":[]},"justifying_aic_and_bic":{"title":"Justifying the AIC and BIC","dependencies":[{"from_tag":"effective-num-parameters","to_tag":"justifying-aic-and-bic","reason":"The AIC and BIC are defined in terms of the effective number of parameters."},{"from_tag":"bayesian-model-comparison","to_tag":"justifying-aic-and-bic","reason":"The BIC is justified as an approximation to Bayesian model comparison."},{"from_tag":"laplace-approximation","to_tag":"justifying-aic-and-bic","reason":"The BIC is justified as an approximation to the Laplace approximation."},{"from_tag":"asymptotics-of-maximum-likelihood","to_tag":"justifying-aic-and-bic","reason":"The AIS is justified in terms of the asymptotic variance of maximum likelihood."},{"from_tag":"bias-variance-decomposition","to_tag":"justifying-aic-and-bic","reason":"The AIS is justified in terms of the asymptotic variance of maximum likelihood."}],"pointers":[]},"dirichlet_distribution":{"title":"Dirichlet distribution","dependencies":[],"pointers":[]},"sequential_maximum_likelihood_estimation":{"title":"Sequential maximum likelihood estimation","dependencies":[{"from_tag":"maximum-likelihood","to_tag":"sequential-maximum-likelihood-estimation","reason":"None"},{"from_tag":"robbins-monro","to_tag":"sequential-maximum-likelihood-estimation","reason":"Sequential maximum likelihood estimation uses the Robbins-Monro procedure."}],"pointers":[{"from_tag":"sequential-maximum-likelihood-estimation","to_tag":"stochastic-gradient-descent","blurb":"* Stochastic gradient descent is a simple and general optimization technique often used in maximum likelihood estimation with large datasets."}]},"multidimensional_scaling":{"title":"Multidimensional scaling","dependencies":[],"pointers":[{"from_tag":"multidimensional-scaling","to_tag":"gaussian-process-latent-variable-model","blurb":"** Gaussian process latent variable models, a Bayesian model similar in spirit to MDS"}]},"tangent_propagation":{"title":"Tangent propagation","dependencies":[{"from_tag":"backpropagation","to_tag":"tangent-propagation","reason":"Tangent propagation is based on the same idea as backpropagation."},{"from_tag":"learning-invariances-in-neural-nets","to_tag":"tangent-propagation","reason":"Tangent propagation is a technique for learning invariances in neural nets."}],"pointers":[{"from_tag":"tangent-propagation","to_tag":"convolutional-nets","blurb":"** building it explicitly into the architecture, as in convolutional nets"},{"from_tag":"tangent-propagation","to_tag":"tikhonov-regularization","blurb":"** Tikhonov regularization, which penalizes instability with respect to noise"}]},"fisher_discriminant_analysis":{"title":"Fisher discriminant analysis","dependencies":[{"from_tag":"gaussian-discriminant-analysis","to_tag":"fisher-discriminant-analysis","reason":"FDA is a generalization of GDA to more than two classes."},{"from_tag":"multiway-classification","to_tag":"fisher-discriminant-analysis","reason":"FDA can be used for multiway classification."}],"pointers":[{"from_tag":"fisher-discriminant-analysis","to_tag":"principal-component-analysis","blurb":"** principal component analysis (PCA)"},{"from_tag":"fisher-discriminant-analysis","to_tag":"factor-analysis","blurb":"** factor analysis"},{"from_tag":"fisher-discriminant-analysis","to_tag":"multidimensional-scaling","blurb":"** multidimensional scaling (MDS)"}]},"sequential_minimal_optimization":{"title":"Sequential minimal optimization","dependencies":[{"from_tag":"svm-optimality-conditions","to_tag":"sequential-minimal-optimization","reason":"Motivating SMO requires the SVM optimality conditions."}],"pointers":[]},"markov_chain_monte_carlo":{"title":"Markov-chain Monte Carlo","dependencies":[{"from_tag":"sampling-methods","to_tag":"markov-chain-monte-carlo","reason":"MCMC is a kind of sampling method."}],"pointers":[{"from_tag":"markov-chain-monte-carlo","to_tag":"gibbs-sampling","blurb":"** Gibbs sampling, where one variable is resampled given the others"},{"from_tag":"markov-chain-monte-carlo","to_tag":"metropolis-hastings","blurb":"** Metropolis-Hastings, a very general technique"}]},"annealed_importance_sampling":{"title":"Annealed importance sampling","dependencies":[{"from_tag":"markov-chain-monte-carlo","to_tag":"annealed-importance-sampling","reason":"AIS is an MCMC algorithm."},{"from_tag":"importance-sampling","to_tag":"annealed-importance-sampling","reason":"AIS gives a weighted sample from the distribution."}],"pointers":[{"from_tag":"annealed-importance-sampling","to_tag":"ais-partition-function","blurb":"* AIS is commonly used to estimate the partition function of a probabilistic model."}]},"bayesian_linear_regression":{"title":"Bayesian linear regression","dependencies":[{"from_tag":"bayesian-parameter-estimation","to_tag":"bayesian-linear-regression","reason":"This is an example of Bayesian parameter estimation."},{"from_tag":"bayesian-decision-theory","to_tag":"bayesian-linear-regression","reason":"Bayesian decision theory tells us how to make predictions in Bayesian linear regression."}],"pointers":[{"from_tag":"bayesian-linear-regression","to_tag":"gaussian-process-regression","blurb":"* Gaussian process regression is a nonparametric analogue of Bayesian linear regression which uses kernels."}]},"em_for_pca":{"title":"EM algorithm for PCA","dependencies":[{"from_tag":"expectation-maximization","to_tag":"em-for-pca","reason":"None"},{"from_tag":"probabilistic-pca","to_tag":"em-for-pca","reason":"EM is a way of fitting the probabilistic PCA model."}],"pointers":[]},"learning_invariances_in_neural_nets":{"title":"Learning invariances in neural nets","dependencies":[{"from_tag":"backpropagation","to_tag":"learning-invariances-in-neural-nets","reason":"We try to modify the backpropagation procedure to encourage invariances."},{"from_tag":"generalization","to_tag":"learning-invariances-in-neural-nets","reason":"The point of learning invariances is to improve generalization performance."}],"pointers":[{"from_tag":"learning-invariances-in-neural-nets","to_tag":"convolutional-nets","blurb":"** building it explicitly into the architecture, as in convolutional nets"},{"from_tag":"learning-invariances-in-neural-nets","to_tag":"tangent-propagation","blurb":"** tangent propagation, which penalizes instability with respect to transformations"},{"from_tag":"learning-invariances-in-neural-nets","to_tag":"tikhonov-regularization","blurb":"** Tikhonov regularization, which penalizes instability with respect to noise"}]},"probabilistic_pca":{"title":"Probabilistic PCA","dependencies":[{"from_tag":"principal-component-analysis","to_tag":"probabilistic-pca","reason":"None"},{"from_tag":"maximum-likelihood","to_tag":"probabilistic-pca","reason":"Fitting probabilistic PCA is done using maximum likelihood."}],"pointers":[{"from_tag":"probabilistic-pca","to_tag":"bayesian-pca","blurb":"* Bayesian PCA is a Bayesian version of pPCA."}]},"bayesian_logistic_regression":{"title":"Bayesian logistic regression","dependencies":[{"from_tag":"logistic-regression","to_tag":"bayesian-logistic-regression","reason":"None"},{"from_tag":"bayesian-linear-regression","to_tag":"bayesian-logistic-regression","reason":"Many of the ideas from Bayesian linear regression transfer to Bayesian logistic regression."},{"from_tag":"laplace-approximation","to_tag":"bayesian-logistic-regression","reason":"The Laplace approximation is a simple way to approximate Bayesian logistic regression."},{"from_tag":"evidence-approximation","to_tag":"bayesian-logistic-regression","reason":"The evidence approximation is a simple way to choose hyperparameters in Bayesian logistic regression."},{"from_tag":"bayesian-decision-theory","to_tag":"bayesian-logistic-regression","reason":"Decision theory tells us how to make predictions from Bayesian parameter estimation."}],"pointers":[{"from_tag":"bayesian-logistic-regression","to_tag":"laplace-approximation","blurb":"** Laplace approximation"},{"from_tag":"bayesian-logistic-regression","to_tag":"variational-logistic-regression","blurb":"** variational Bayes"}]},"bayesian_pca":{"title":"Bayesian PCA","dependencies":[{"from_tag":"probabilistic-pca","to_tag":"bayesian-pca","reason":"Bayesian PCA is an elaboration of probabilistic PCA."},{"from_tag":"bayesian-linear-regression","to_tag":"bayesian-pca","reason":"Bayesian PCA is based on similar ideas to Bayesian linear regression."},{"from_tag":"evidence-approximation","to_tag":"bayesian-pca","reason":"The evidence approximation can be used to select the dimensionality."},{"from_tag":"bayes-param-multivariate-gaussian","to_tag":"bayesian-pca","reason":"The same ideas are required for modeling the variance parameters."}],"pointers":[{"from_tag":"bayesian-pca","to_tag":"gibbs-sampling","blurb":"* We can perform inference in this model using Gibbs sampling."}]},"sampling_importance_resampling":{"title":"Sampling-importance-resampling","dependencies":[{"from_tag":"importance-sampling","to_tag":"sampling-importance-resampling","reason":"Importance sampling is one step of SIR."}],"pointers":[]},"pac_learning":{"title":"PAC learning","dependencies":[{"from_tag":"generalization","to_tag":"pac-learning","reason":"PAC learning is a way of analyzing the generalization performance of learning algorithms."}],"pointers":[{"from_tag":"pac-learning","to_tag":"pac-learning-proofs","blurb":"* Proofs of the results presented here"}]},"evidence_approximation":{"title":"The evidence approximation","dependencies":[{"from_tag":"bayesian-model-comparison","to_tag":"evidence-approximation","reason":"The evidence approximation is a way of approximating Bayesian model comparison."}],"pointers":[{"from_tag":"evidence-approximation","to_tag":"variational-bayes","blurb":"* The evidence approximation requires integrating out the model parameters. Variational Bayes gives a way of doing this."}]},"hidden_markov_models":{"title":"Hidden Markov models","dependencies":[{"from_tag":"markov-models","to_tag":"hidden-markov-models","reason":"Markov models are a component of HMMs."}],"pointers":[]},"importance_sampling":{"title":"Importance sampling","dependencies":[{"from_tag":"sampling-methods","to_tag":"importance-sampling","reason":"None"}],"pointers":[]},"gibbs_as_mh":{"title":"Gibbs sampling as a special case of Metropolis-Hastings","dependencies":[{"from_tag":"gibbs-sampling","to_tag":"gibbs-as-mh","reason":"None"},{"from_tag":"metropolis-hastings","to_tag":"gibbs-as-mh","reason":"None"}],"pointers":[]},"generalization":{"title":"Generalization","dependencies":[{"from_tag":"linear-regression","to_tag":"generalization","reason":"Linear regression is an instructive example highlighting various issues of generalization."}],"pointers":[{"from_tag":"generalization","to_tag":"model-selection","blurb":"** model selection"},{"from_tag":"generalization","to_tag":"regularization","blurb":"** regularization"},{"from_tag":"generalization","to_tag":"bias-variance-decomposition","blurb":"** For linear regression, generalization error can be determined analytically, and breaks down exactly into a sum of bias and variance terms. This provides a useful intuition for other models as well."},{"from_tag":"generalization","to_tag":"pac-learning","blurb":"** Probably Approximately Correct (PAC) learning, which analyzes whether an algorithm usually learns a good-enough model"},{"from_tag":"generalization","to_tag":"vc-dimension","blurb":"** VC dimension, a quantity which characterizes the complexity of a continuously-parameterized model"},{"from_tag":"generalization","to_tag":"structural-risk-minimization","blurb":"** Structural risk minimization, a way of controlling overfitting by defining a nested sequence of models of increasing complexity"}]},"kernel_svm":{"title":"Kernel SVM","dependencies":[{"from_tag":"svm-optimality-conditions","to_tag":"kernel-svm","reason":"Kernelizing the SVM requires deriving the optimality conditions."},{"from_tag":"kernel-trick","to_tag":"kernel-svm","reason":"None"}],"pointers":[{"from_tag":"kernel-svm","to_tag":"sequential-minimal-optimization","blurb":"* The kernel SVM can be optimized with the sequential minimal optimization (SMO) algorithm."},{"from_tag":"kernel-svm","to_tag":"constructing-kernels","blurb":"* Techniques for constructing kernels"},{"from_tag":"kernel-svm","to_tag":"kernel-ridge-regression","blurb":"** kernel ridge regression"},{"from_tag":"kernel-svm","to_tag":"gaussian-process-regression","blurb":"** Gaussian process regression"}]},"variational_inference_convex_duality":{"title":"Variational inference and convex duality","dependencies":[{"from_tag":"variational-inference","to_tag":"variational-inference-convex-duality","reason":"None"}],"pointers":[]},"support_vector_regression":{"title":"Support vector regression","dependencies":[{"from_tag":"support-vector-machine","to_tag":"support-vector-regression","reason":"None"},{"from_tag":"linear-regression","to_tag":"support-vector-regression","reason":"None"}],"pointers":[]},"hamiltonian_monte_carlo":{"title":"Hamiltonian Monte Carlo","dependencies":[{"from_tag":"slice-sampling","to_tag":"hamiltonian-monte-carlo","reason":"HMC is a special case of slice sampling."},{"from_tag":"metropolis-hastings","to_tag":"hamiltonian-monte-carlo","reason":"HMC is an example of an M-H sampler."}],"pointers":[]},"binary_linear_classifiers":{"title":"Binary linear classifiers","dependencies":[{"from_tag":"linear-regression","to_tag":"binary-linear-classifiers","reason":"Linear regression provides useful intuitions for thinking about binary linear classification."}],"pointers":[{"from_tag":"binary-linear-classifiers","to_tag":"perceptron","blurb":"** perceptron"},{"from_tag":"binary-linear-classifiers","to_tag":"logistic-regression","blurb":"** logistic regression"},{"from_tag":"binary-linear-classifiers","to_tag":"gaussian-discriminant-analysis","blurb":"** Gaussian discriminant analysis"},{"from_tag":"binary-linear-classifiers","to_tag":"naive-bayes","blurb":"** naive Bayes"},{"from_tag":"binary-linear-classifiers","to_tag":"linear-regression","blurb":"** real-valued"},{"from_tag":"binary-linear-classifiers","to_tag":"multiway-classification","blurb":"** categorical"},{"from_tag":"binary-linear-classifiers","to_tag":"generalization","blurb":"* We want our classifier to generalize well to new data, not just perform well on the data it's already seen."},{"from_tag":"binary-linear-classifiers","to_tag":"regularization","blurb":"** regularization, where overly complex solutions are penalized"},{"from_tag":"binary-linear-classifiers","to_tag":"model-selection","blurb":"** model selection"},{"from_tag":"binary-linear-classifiers","to_tag":"basis-function-expansions","blurb":"** basis function expansions"},{"from_tag":"binary-linear-classifiers","to_tag":"feed-forward-neural-nets","blurb":"** neural networks"},{"from_tag":"binary-linear-classifiers","to_tag":"kernel-svm","blurb":"** kernel methods"}]},"effective_num_parameters":{"title":"Effective number of parameters","dependencies":[{"from_tag":"smoother-matrix","to_tag":"effective-num-parameters","reason":"The effective number of parameters is the trace of the smoother matrix."},{"from_tag":"generalization","to_tag":"effective-num-parameters","reason":"The effective number of parameters is a way of analyzing generalization performance."}],"pointers":[{"from_tag":"effective-num-parameters","to_tag":"vc-dimension","blurb":"* VC dimension is a more general way of determining the complexity of a model class which often coincides with the number of parameters."}]},"regularization":{"title":"Regularization","dependencies":[{"from_tag":"lasso","to_tag":"regularization","reason":"LASSO is an instructive example of regularization."},{"from_tag":"generalization","to_tag":"regularization","reason":"Regularization is a strategy for improving generalization performance."}],"pointers":[{"from_tag":"regularization","to_tag":"tikhonov-regularization","blurb":"** Tikhonov regularization, which penalizes the sensitivity of the model's outputs to noises in the inputs"},{"from_tag":"regularization","to_tag":"structural-risk-minimization","blurb":"* Regularization can equivalently be viewed as adding constraints to a model, a view taken in structural risk minimization"}]},"bayesian_decision_theory":{"title":"Bayesian decision theory","dependencies":[{"from_tag":"bayesian-parameter-estimation","to_tag":"bayesian-decision-theory","reason":"We use Bayesian parameter estimation to get the posterior on which we base our decisions."}],"pointers":[]},"asymptotics_of_maximum_likelihood":{"title":"Asymptotics of maximum likelihood","dependencies":[{"from_tag":"maximum-likelihood","to_tag":"asymptotics-of-maximum-likelihood","reason":"None"}],"pointers":[]},"laplace_approximation":{"title":"The Laplace approximation","dependencies":[{"from_tag":"asymptotics-of-maximum-likelihood","to_tag":"laplace-approximation","reason":"The Laplace approximation is justified in terms of the asymptotic behavior of maximum likelihood."},{"from_tag":"bayesian-parameter-estimation","to_tag":"laplace-approximation","reason":"The Laplace approximation is an approximation to Bayesian likelihood estimation."},{"from_tag":"bayesian-model-comparison","to_tag":"laplace-approximation","reason":"The Laplace approximation is a method for Bayesian model comparison."}],"pointers":[{"from_tag":"laplace-approximation","to_tag":"variational-bayes","blurb":"** variational Bayes"},{"from_tag":"laplace-approximation","to_tag":"justifying-aic-and-bic","blurb":"* The Bayesian information criterion (BIC) can be justified in terms of the Laplace approximation."},{"from_tag":"laplace-approximation","to_tag":"variational-bayes","blurb":"** variational Bayes"},{"from_tag":"laplace-approximation","to_tag":"variational-logistic-regression","blurb":"** Gaussian lower bound to the logistic function"}]},"sampling_methods":{"title":"Sampling methods","dependencies":[],"pointers":[{"from_tag":"sampling-methods","to_tag":"gibbs-sampling","blurb":"** Gibbs sampling, a generic and widely applicable sampling algorithm"},{"from_tag":"sampling-methods","to_tag":"metropolis-hastings","blurb":"** Metropolis-Hastings algorithm, which is very general"},{"from_tag":"sampling-methods","to_tag":"markov-chain-monte-carlo","blurb":"** Markov chain Monte Carlo (MCMC)"},{"from_tag":"sampling-methods","to_tag":"sampling-partition-function","blurb":"** estimating the partition function"},{"from_tag":"sampling-methods","to_tag":"variational-inference","blurb":"** variational inference, which tries to approximate an intractable posterior distribution with a tractable one"}]},"mixture_of_bernoullis":{"title":"Mixture of Bernoullis","dependencies":[{"from_tag":"naive-bayes","to_tag":"mixture-of-bernoullis","reason":"Mixture of Bernoullis has the same form as naive Bayes."}],"pointers":[{"from_tag":"mixture-of-bernoullis","to_tag":"gaussian-mixtures-vs-k-means","blurb":"** mixture of Gaussians, a probabilistic model for continuous data"},{"from_tag":"mixture-of-bernoullis","to_tag":"mixture-of-bernoullis","blurb":"** mixture of Bernoullis, a probabilistic model for discrete data"}]},"tikhonov_regularization":{"title":"Tikhonov regularization","dependencies":[{"from_tag":"learning-invariances-in-neural-nets","to_tag":"tikhonov-regularization","reason":"Tikhonov regularization is a way of learning invariances in neural nets."}],"pointers":[{"from_tag":"tikhonov-regularization","to_tag":"convolutional-nets","blurb":"** building it explicitly into the architecture, as in convolutional nets"},{"from_tag":"tikhonov-regularization","to_tag":"tangent-propagation","blurb":"** tangent propagation, which penalizes instability with respect to transformations"}]},"model_selection":{"title":"Model selection","dependencies":[{"from_tag":"generalization","to_tag":"model-selection","reason":"One of our desiderata for model selection is that the model generalize well to new data."},{"from_tag":"basis-function-expansions","to_tag":"model-selection","reason":"Basis function expansions are an instructive case of model selection."}],"pointers":[{"from_tag":"model-selection","to_tag":"bayesian-model-comparison","blurb":"*** Bayesian model comparison"}]},"soft_weight_sharing_neural_nets":{"title":"Soft weight sharing in neural nets","dependencies":[{"from_tag":"weight-decay","to_tag":"soft-weight-sharing-neural-nets","reason":"Soft weight sharing is an alternative regularization scheme for neural nets based on weight decay."},{"from_tag":"mixture-of-gaussians","to_tag":"soft-weight-sharing-neural-nets","reason":"Soft weight sharing is based on mixture of Gaussians models."}],"pointers":[]},"stochastic_gradient_descent":{"title":"Stochastic gradient descent","dependencies":[{"from_tag":"robbins-monro","to_tag":"stochastic-gradient-descent","reason":"The Robbins-Monro procedure is one way of ensuring that stochastic gradient descent converges."}],"pointers":[]},"variational_linear_regression":{"title":"Variational linear regression","dependencies":[{"from_tag":"bayesian-linear-regression","to_tag":"variational-linear-regression","reason":"None"},{"from_tag":"variational-bayes","to_tag":"variational-linear-regression","reason":"None"}],"pointers":[{"from_tag":"variational-linear-regression","to_tag":"variational-logistic-regression","blurb":"* Variational logistic regression is the analogue for classification."}]},"kernel_ridge_regression":{"title":"Kernel ridge regression","dependencies":[{"from_tag":"kernel-trick","to_tag":"kernel-ridge-regression","reason":"None"}],"pointers":[{"from_tag":"kernel-ridge-regression","to_tag":"gaussian-process-regression","blurb":"* Gaussian process regression is a Bayesian version of kernel ridge regression."},{"from_tag":"kernel-ridge-regression","to_tag":"kernel-svm","blurb":"* Kernel support vector machines are a classification algorithm that uses kernels."}]},"variational_mixture_of_gaussians":{"title":"Variational mixture of Gaussians","dependencies":[{"from_tag":"variational-bayes","to_tag":"variational-mixture-of-gaussians","reason":"None"},{"from_tag":"mixture-of-gaussians","to_tag":"variational-mixture-of-gaussians","reason":"None"},{"from_tag":"bayes-param-multivariate-gaussian","to_tag":"variational-mixture-of-gaussians","reason":"The posterior over parameters for a multivariate Gaussian is part of the mean field update rule for this model."}],"pointers":[]},"transformation_method":{"title":"Transformation method","dependencies":[{"from_tag":"sampling-methods","to_tag":"transformation-method","reason":"None"}],"pointers":[{"from_tag":"transformation-method","to_tag":"rejection-sampling","blurb":"** rejection sampling"},{"from_tag":"transformation-method","to_tag":"slice-sampling","blurb":"** slice sampling, an iterative method"},{"from_tag":"transformation-method","to_tag":"sampling-gaussian","blurb":"** the polar coordinates trick for sampling from a Gaussian"}]},"variational_logistic_regression":{"title":"Variational logistic regression","dependencies":[{"from_tag":"bayesian-logistic-regression","to_tag":"variational-logistic-regression","reason":"This is an approximate inference algorithm for Bayesian logistic regression."},{"from_tag":"variational-bayes","to_tag":"variational-logistic-regression","reason":"None"}],"pointers":[]},"constructing_kernels":{"title":"Constructing kernels","dependencies":[{"from_tag":"kernel-trick","to_tag":"constructing-kernels","reason":"None"}],"pointers":[{"from_tag":"constructing-kernels","to_tag":"fisher-kernel","blurb":"* Fisher kernels are a general recipe for obtaining a kernel from a generative model."}]},"variational_exponential_family":{"title":"Variational inference and exponential families","dependencies":[{"from_tag":"variational-inference","to_tag":"variational-exponential-family","reason":"None"}],"pointers":[]},"linear_regression_as_maximum_likelihood":{"title":"Linear regression as maximum likelihood","dependencies":[{"from_tag":"linear-regression","to_tag":"linear-regression-as-maximum-likelihood","reason":"We're interpreting linear regression as maximum likelihood."},{"from_tag":"maximum-likelihood","to_tag":"linear-regression-as-maximum-likelihood","reason":"We're interpreting linear regression as maximum likelihood."}],"pointers":[{"from_tag":"linear-regression-as-maximum-likelihood","to_tag":"bayesian-linear-regression","blurb":"** Bayesian linear regression"},{"from_tag":"linear-regression-as-maximum-likelihood","to_tag":"generalized-linear-models","blurb":"* Linear regression is a kind of generalized linear model."}]},"multinomial_logistic_regression":{"title":"Multinomial logistic regression","dependencies":[{"from_tag":"logistic-regression","to_tag":"multinomial-logistic-regression","reason":"None"},{"from_tag":"multiway-classification","to_tag":"multinomial-logistic-regression","reason":"Multinomial logistic regression is a multiway classification algorithm."}],"pointers":[{"from_tag":"multinomial-logistic-regression","to_tag":"naive-bayes","blurb":"** naive Bayes"},{"from_tag":"multinomial-logistic-regression","to_tag":"generative-vs-discriminative","blurb":"* Multinomial logistic regression is a discriminative model."},{"from_tag":"multinomial-logistic-regression","to_tag":"generalized-linear-models","blurb":"* Multinomial logistic regression is a kind of generalized linear model."}]},"em_gaussian_mixtures":{"title":"EM for Gaussian mixtures","dependencies":[{"from_tag":"expectation-maximization","to_tag":"em-gaussian-mixtures","reason":"None"},{"from_tag":"mixture-of-gaussians","to_tag":"em-gaussian-mixtures","reason":"None"}],"pointers":[{"from_tag":"em-gaussian-mixtures","to_tag":"k-means","blurb":"* The EM algorithm for Gaussian mixtures is analogous to k-means."},{"from_tag":"em-gaussian-mixtures","to_tag":"variational-mixture-of-gaussians","blurb":"* Variational inference in a Bayesian mixture of Gaussians model has a similar form to EM"}]},"multiway_classification":{"title":"Multiway classification","dependencies":[{"from_tag":"binary-linear-classifiers","to_tag":"multiway-classification","reason":"A common way to fit multiway classifiers is to fit a series of binary classifiers."}],"pointers":[{"from_tag":"multiway-classification","to_tag":"multinomial-logistic-regression","blurb":"** multinomial logistic regression"},{"from_tag":"multiway-classification","to_tag":"multiclass-svm","blurb":"** multiclass support vector machines (SVMs)"},{"from_tag":"multiway-classification","to_tag":"linear-regression","blurb":"** real-valued"},{"from_tag":"multiway-classification","to_tag":"binary-linear-classifiers","blurb":"** binary"},{"from_tag":"multiway-classification","to_tag":"generalization","blurb":"* We want our classifier to generalize well to new data, not just perform well on the data it's already seen."},{"from_tag":"multiway-classification","to_tag":"regularization","blurb":"** regularization, where overly complex solutions are penalized"},{"from_tag":"multiway-classification","to_tag":"model-selection","blurb":"** model selection"},{"from_tag":"multiway-classification","to_tag":"basis-function-expansions","blurb":"** basis function expansions"},{"from_tag":"multiway-classification","to_tag":"feed-forward-neural-nets","blurb":"** neural networks"},{"from_tag":"multiway-classification","to_tag":"kernel-svm","blurb":"** kernel methods"}]},"bias_variance_decomposition":{"title":"Bias-variance decomposition","dependencies":[{"from_tag":"linear-regression","to_tag":"bias-variance-decomposition","reason":"The bias-variance decomposition holds exactly in the case of linear regression."},{"from_tag":"generalization","to_tag":"bias-variance-decomposition","reason":"The bias-variance decomposition is a way of analyzing generalization error."}],"pointers":[{"from_tag":"bias-variance-decomposition","to_tag":"effective-num-parameters","blurb":"* The variance term depends on the effective number of parameters of a model."}]},"linear_regression_multiple_outputs":{"title":"Linear regression with multiple outputs","dependencies":[{"from_tag":"linear-regression-as-maximum-likelihood","to_tag":"linear-regression-multiple-outputs","reason":"The maximum likelihood interpretation of linear regression tells us how to generalize it to multiple outputs."}],"pointers":[]},"principal_component_analysis":{"title":"Principal component analysis","dependencies":[],"pointers":[{"from_tag":"principal-component-analysis","to_tag":"pca-preprocessing","blurb":"** as a preprocessing step for supervised learning; the idea is to improve generalization or computational efficiency by reducing the dimensionality of the inputs"},{"from_tag":"principal-component-analysis","to_tag":"probabilistic-pca","blurb":"** probabilistic PCA, where the same algorithm is interpreted as fitting a generative model"},{"from_tag":"principal-component-analysis","to_tag":"factor-analysis","blurb":"** factor analysis, another related generative model where each input dimension can have a separate noise variance"},{"from_tag":"principal-component-analysis","to_tag":"bayesian-pca","blurb":"** Bayesian PCA"},{"from_tag":"principal-component-analysis","to_tag":"kernel-pca","blurb":"** kernel PCA, which implicitly maps the data to a high-dimensional space before computing the PCA vectors"},{"from_tag":"principal-component-analysis","to_tag":"em-for-pca","blurb":"* For very high-dimensional spaces, directly computing the leading eigenvectors is impractical. In these situations, expectation-maximization (EM) can be more practical."},{"from_tag":"principal-component-analysis","to_tag":"fisher-discriminant-analysis","blurb":"* Fisher discriminant analysis is another projection similar to PCA, but which uses class labels also."}]},"expectation_maximization":{"title":"Expectation-maximization algorithm","dependencies":[{"from_tag":"maximum-likelihood","to_tag":"expectation-maximization","reason":"EM is a way of finding (approximate) maximum likelihood parameters."}],"pointers":[{"from_tag":"expectation-maximization","to_tag":"em-gaussian-mixtures","blurb":"** Gaussian mixture models"},{"from_tag":"expectation-maximization","to_tag":"em-variational-interpretation","blurb":"** EM can be viewed as coordinate ascent on a lower bound of the data likelihood."}]},"maximum_likelihood_in_exponential_families":{"title":"Maximum likelihood in exponential families","dependencies":[{"from_tag":"maximum-likelihood","to_tag":"maximum-likelihood-in-exponential-families","reason":"None"}],"pointers":[{"from_tag":"maximum-likelihood-in-exponential-families","to_tag":"bayes-param-exp-fam","blurb":"* Bayesian parameter estimation often has a convenient form in exponential families."}]},"k_means":{"title":"K-means","dependencies":[],"pointers":[{"from_tag":"k-means","to_tag":"gaussian-mixtures-vs-k-means","blurb":"** mixture of Gaussians, a probabilistic model for continuous data"},{"from_tag":"k-means","to_tag":"mixture-of-bernoullis","blurb":"** mixture of Bernoullis, a probabilistic model for discrete data"},{"from_tag":"k-means","to_tag":"em-gaussian-mixtures","blurb":"* K-means is analogous to the EM algorithm for fitting Gaussian mixtures"}]},"gaussian_processes":{"title":"Gaussian processes","dependencies":[{"from_tag":"kernel-trick","to_tag":"gaussian-processes","reason":"Gaussian processes are defined in terms of kernels."}],"pointers":[{"from_tag":"gaussian-processes","to_tag":"gaussian-process-regression","blurb":"** regression"},{"from_tag":"gaussian-processes","to_tag":"gaussian-process-classification","blurb":"** classification"},{"from_tag":"gaussian-processes","to_tag":"constructing-kernels","blurb":"* Techniques for constructing kernel functions"}]},"pca_preprocessing":{"title":"PCA preprocessing","dependencies":[{"from_tag":"principal-component-analysis","to_tag":"pca-preprocessing","reason":"None"},{"from_tag":"linear-regression","to_tag":"pca-preprocessing","reason":"Linear regression is an instructive example."},{"from_tag":"generalization","to_tag":"pca-preprocessing","reason":"PCA preprocessing is a way of improving generalization."}],"pointers":[{"from_tag":"pca-preprocessing","to_tag":"feed-forward-neural-nets","blurb":"** Neural nets, which can adapt the representation as learning progresses, rather than fixing it at the start"}]},"backpropagation":{"title":"Backpropagation","dependencies":[{"from_tag":"feed-forward-neural-nets","to_tag":"backpropagation","reason":"Backpropagation is an algorithm for training feed-forward neural nets."},{"from_tag":"stochastic-gradient-descent","to_tag":"backpropagation","reason":"Backpropagation is a kind of gradient descent."}],"pointers":[{"from_tag":"backpropagation","to_tag":"backpropagation-second-order","blurb":"* Backpropagation can be used to compute second derivatives as well."}]},"uninformative_priors":{"title":"Uninformative priors","dependencies":[{"from_tag":"bayesian-parameter-estimation","to_tag":"uninformative-priors","reason":"Uninformative priors are normally used for Bayesian parameter estimation."}],"pointers":[]},"student_t_distribution":{"title":"Student-t distribution","dependencies":[],"pointers":[{"from_tag":"student-t-distribution","to_tag":"bayes-param-multivariate-gaussian","blurb":"* The student-t distribution is the predictive distribution for Bayesian parameter estimation of Gaussian distributions with unknown variance."}]}}